# -*- coding: utf-8 -*-
"""USER-PROFILING-SEGMENTATION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J1vr134MxbGlsRdpWrexQLXq-7ZEYinK

User Profiling and Segmentation

User profiling and segmentation are powerful techniques that enable data professionals to understand their user base in-depth and tailor their strategies to meet diverse user needs. Below is the process we can follow for the task of User Profiling and Segmentation:

1. Determine what you aim to achieve with user profiling and segmentation, such as improving customer service, personalized marketing, or product recommendation.
2. Collect data from various sources, including user interactions on websites/apps, transaction histories, social media activity, and demographic information.
3. Create new features that capture relevant user behaviours and preferences. It may involve aggregating transaction data, calculating the frequency of activities, or extracting patterns from usage logs.
4. Select appropriate segmentation techniques.
5. For each segment identified, create user profiles that summarize the key characteristics and behaviours of users in that segment. User profiling is the process of creating a detailed description of someone based on their personal information, characteristics, interests, online behavior, and other relevant attributes.
6. The given dataset comprises a diverse set of user metrics collected from an online platform, consisting of 1,000 user profiles with 16 distinct features. These features include demographics (age, gender, income level, and education), online behavior (likes and reactions, followed accounts, device usage), engagement with content (time spent online during weekdays and weekends, click-through rates, conversion rates), interaction with ads (ad interaction time), and user interests.
7. Your task is to develop a robust user profiling and segmentation system that leverages machine learning and data analysis techniques to categorize users into distinct segments. By analyzing user interaction data, demographic information, and engagement metrics, identify meaningful patterns and clusters within the user base. The ultimate goal is to enable businesses to tailor their advertising campaigns to the identified segments, thereby increasing ad relevance, user engagement, and conversion rates while optimizing ad spend.
"""

#Step 1: Load and Inspect Data
import pandas as pd

# Load the dataset
df = pd.read_csv('/content/user_profiles_for_ads.csv')

# Display basic info and first few rows
print("Shape:", df.shape)
df.head()

#Step 2: Basic EDA
# Check data types and missing values
df.info()

# Count missing values per column
df.isnull().sum()

#Step 3: Summary Statistics and Value Counts
# Descriptive statistics for numeric columns
df.describe()

# Value counts for categorical columns (if any)
categorical_cols = df.select_dtypes(include='object').columns
for col in categorical_cols:
    print(f"\nValue counts for {col}:\n", df[col].value_counts())

#Step 4: Data Preprocessing
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Example: Fill missing values (can be modified based on your data)
df.fillna(df.mean(numeric_only=True), inplace=True)

# Encode categorical variables (if present)
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])

# Scale numerical features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df)
df_scaled = pd.DataFrame(scaled_features, columns=df.columns)
df_scaled.head()

# Step 5: Dimensionality Reduction
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
pca_result = pca.fit_transform(df_scaled)

# Visualize PCA results
plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)
plt.title("PCA Projection")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()

# Step 6: Clustering for Segmentation (KMeans)
from sklearn.cluster import KMeans

# Find optimal number of clusters using Elbow method
inertia = []
k_range = range(2, 11)
for k in k_range:
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(df_scaled)
    inertia.append(km.inertia_)

plt.plot(k_range, inertia, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal K')
plt.show()

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

sse = []
k_range = range(1, 11)
for k in k_range:
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(df_scaled)
    sse.append(km.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(k_range, sse, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of clusters (k)')
plt.ylabel('SSE (Inertia)')
plt.grid(True)
plt.show()

#Step 7: Apply KMeans and Assign Segments
# Choose the best K (e.g., from elbow)
k = 4
kmeans = KMeans(n_clusters=k, random_state=42)
df_scaled['Segment'] = kmeans.fit_predict(df_scaled)

# Add segment labels to original data
df['Segment'] = df_scaled['Segment']
df.head()

optimal_k = 4  # Update based on elbow method result
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
df_scaled['cluster'] = kmeans.fit_predict(df_scaled)

# Visualize clusters on PCA projection
plt.figure(figsize=(8, 5))
scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=df_scaled['cluster'], cmap='viridis', alpha=0.6)
plt.title("User Segments (KMeans Clusters)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.colorbar(scatter, label='Cluster')
plt.grid(True)
plt.show()

#Step 8: Profile Each Segment
# Average values per segment
segment_profiles = df.groupby('Segment').mean(numeric_only=True)

# Count of users in each segment
segment_counts = df['Segment'].value_counts().sort_index()

print("Segment Counts:\n", segment_counts)
segment_profiles

#  User Profiling per Cluster
df_with_clusters = df.copy()
df_with_clusters['cluster'] = df_scaled['cluster']

# Get average stats per cluster
cluster_profiles = df_with_clusters.groupby('cluster').mean(numeric_only=True)
print("Cluster Profile Summaries:")
print(cluster_profiles)

# Value counts for categorical features per cluster (optional)
for col in categorical_cols:
    print(f"\n{col} distribution by cluster:")
    print(df_with_clusters.groupby('cluster')[col].value_counts(normalize=True).unstack().round(2))

# Step 9: Visualize Segment Characteristics
import seaborn as sns

# Pairplot to view segment separation
sns.pairplot(df_scaled.sample(300), hue='Segment', palette='tab10')

print(df.columns.tolist())

df['Segment'] = df_scaled['Segment']

# Group by segment

summary = df.groupby('Segment').agg({
    'Age': ['min', 'max', 'mean'],
    'Click-Through Rates (CTR)': 'mean',
    'Device Usage': lambda x: x.mode().iloc[0] if not x.mode().empty else 'N/A',
    'Top Interests': lambda x: x.mode().iloc[0] if not x.mode().empty else 'N/A',
    'Segment': 'count'
})

# Rename columns for clarity
summary.columns = ['Age Min', 'Age Max', 'Avg Age', 'Avg CTR', 'Top Device', 'Top Interest', 'User Count']
summary = summary.reset_index()

summary

print(df.columns.tolist())

df['Segment'] = df_scaled['Segment']

# Properly grouped segment summary with correct column names
summary = df.groupby('Segment').agg({
    'Age': ['min', 'max', 'mean'],
    'Click-Through Rates (CTR)': 'mean',
    'Device Usage': lambda x: x.mode().iloc[0] if not x.mode().empty else 'N/A',
    'Top Interests': lambda x: x.mode().iloc[0] if not x.mode().empty else 'N/A',
    'Segment': 'count'
})

# Rename columns for readability
summary.columns = ['Age Min', 'Age Max', 'Avg Age', 'Avg CTR', 'Top Device', 'Top Interest', 'User Count']
summary = summary.reset_index()
summary

from sklearn.preprocessing import StandardScaler, LabelEncoder
import numpy as np

# Drop User ID and non-numeric columns not suitable for clustering
df_clustering = df.drop(columns=['User ID', 'Location', 'Language', 'Top Interests'])

# Encode categorical columns
label_cols = ['Age', 'Gender', 'Education Level', 'Device Usage', 'Income Level']
for col in label_cols:
    df_clustering[col] = LabelEncoder().fit_transform(df_clustering[col])

# Scale the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_clustering)

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

kmeans = KMeans(n_clusters=4, random_state=42)
kmeans_labels = kmeans.fit_predict(scaled_data)
print("KMeans Silhouette Score:", silhouette_score(scaled_data, kmeans_labels))

# Frequency-based feature: Engagement score
# Advanced Feature Engineering
df['Engagement_Score'] = (df['Likes and Reactions'] + df['Followed Accounts']) / \
                         (df['Time Spent Online (hrs/weekday)'] + df['Time Spent Online (hrs/weekend)'] + 1)

# Pattern-based feature: Weekend vs Weekday engagement ratio
df['Weekend_Weekday_Ratio'] = df['Time Spent Online (hrs/weekend)'] / (df['Time Spent Online (hrs/weekday)'] + 1)

# Binary encoding for high/low income
df['Income_High'] = df['Income Level'].apply(lambda x: 1 if x > df['Income Level'].median() else 0)

# Segment Profiling
# Add cluster labels to the dataset (after clustering)
df['Cluster'] = kmeans.labels_

# Group and describe each segment
for cluster_id in sorted(df['Cluster'].unique()):
    print(f"\n--- Cluster {cluster_id} ---")
    cluster_data = df[df['Cluster'] == cluster_id]
    print("Size:", len(cluster_data))
    print(cluster_data.describe(include='all').T[['mean', 'std', 'min', 'max']])

import seaborn as sns
import matplotlib.pyplot as plt

# Visualize key variables across clusters
sns.boxplot(data=df, x='Cluster', y='Engagement_Score')
plt.title('Engagement Score by Cluster')
plt.show()

# Ad Optimization Insight
# Hypothetical: Average CTR and Conversion per segment
segment_summary = df.groupby('Cluster')[['Click-Through Rates (CTR)', 'Conversion Rates']].mean()
print(segment_summary)

# Use case:
# This helps determine which segments are most responsive to ads
# → You can allocate more ad budget to high-conversion segments

from sklearn.cluster import DBSCAN

from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
import numpy as np

# Try looser settings
dbscan = DBSCAN(eps=1.0, min_samples=3)
db_labels = dbscan.fit_predict(scaled_data)

# Check cluster label distribution
print("DBSCAN Labels:", np.unique(db_labels, return_counts=True))

# Only calculate silhouette if there are valid clusters
if len(set(db_labels)) > 1 and len(set(db_labels)) != 1 + (1 if -1 in db_labels else 0):
    score = silhouette_score(scaled_data[db_labels != -1], db_labels[db_labels != -1])
    print("DBSCAN Silhouette Score (ignoring noise):", score)
else:
    print("DBSCAN found only noise or a single cluster – adjust `eps` or `min_samples`.")

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

X = scaled_data
y = kmeans_labels

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))

# Assign cluster labels to the original data
df['Cluster'] = kmeans_labels

# Group and summarize
cluster_summary = df.groupby('Cluster').mean(numeric_only=True)
print(cluster_summary)

# Convert age groups to numeric midpoints
def age_range_to_midpoint(age_range):
    try:
        lower, upper = map(int, age_range.split('-'))
        return (lower + upper) / 2
    except:
        return None  # or handle unexpected formats

df['Age_numeric'] = df['Age'].apply(age_range_to_midpoint)

features_to_scale = [
    'Age_numeric',  # use this instead of 'Age'
    'Likes and Reactions',
    'Followed Accounts',
    'Time Spent Online (hrs/weekday)',
    'Time Spent Online (hrs/weekend)',
    'Click-Through Rates (CTR)',
    'Conversion Rates',
    'Ad Interaction Time (sec)',
    'Income Level'
]

def age_range_to_midpoint(age_str):
    try:
        lower, upper = map(int, age_str.split('-'))
        return (lower + upper) / 2
    except:
        return None

df['Age_numeric'] = df['Age'].apply(age_range_to_midpoint)

def income_range_to_midpoint(income_str):
    try:
        # Remove 'k' and split
        parts = income_str.replace('k', '').split('-')
        lower, upper = map(int, parts)
        return ((lower + upper) / 2) * 1000
    except:
        return None

df['Income_numeric'] = df['Income Level'].apply(income_range_to_midpoint)

features_to_scale = [
    'Age_numeric',
    'Likes and Reactions',
    'Followed Accounts',
    'Time Spent Online (hrs/weekday)',
    'Time Spent Online (hrs/weekend)',
    'Click-Through Rates (CTR)',
    'Conversion Rates',
    'Ad Interaction Time (sec)',
    'Income_numeric'
]

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X = scaler.fit_transform(df[features_to_scale])

db_labels = dbscan.fit_predict(scaled_data)

df['cluster'] = db_labels

df['segment_name'] = df['cluster'].map(cluster_names)

# Assign names to clusters after analyzing their characteristics
cluster_names = {
    0: "Ad Enthusiasts",
    1: "Weekend Browsers",
    2: "High Spenders",
    3: "Passive Users"
}

df['segment_name'] = df['cluster'].map(cluster_names)

df['segment_name'] = df['cluster'].map(cluster_names)

from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans

# Replace NaNs with the mean of each column
imputer = SimpleImputer(strategy='mean')
X_clean = imputer.fit_transform(X)  # Still a NumPy array

# Now cluster
df['cluster'] = KMeans(n_clusters=4, random_state=42).fit_predict(X_clean)

import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans

# Step 1: Convert X to DataFrame if it's not already
X_df = pd.DataFrame(X)

# Step 2: Drop columns that are entirely NaN
X_df = X_df.dropna(axis=1, how='all')

# Step 3: Impute remaining missing values
imputer = SimpleImputer(strategy='mean')
X_clean = imputer.fit_transform(X_df)

# Step 4: Perform clustering
df['cluster'] = KMeans(n_clusters=4, random_state=42).fit_predict(X_clean)

print(df)